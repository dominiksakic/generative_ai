{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMCj10fscLHaN9OVA4BxHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/generative_ai/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSji_GPQH2dA",
        "outputId": "6b0b9d4b-e896-410e-d3e9-967767ee62b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-01 12:09:56--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  16.2MB/s    in 12s     \n",
            "\n",
            "2025-06-01 12:10:09 (6.54 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download data\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Prepare data\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5mnl_boICAg",
        "outputId": "b4e10124-423e-44cc-f534-2ad8eb669a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up language dataset\n",
        "def prepare_lm_dataset(text_batch):\n",
        "  vectorized_sequences = text_vectorization(text_batch)\n",
        "  x = vectorized_sequences[:, :-1]\n",
        "  y = vectorized_sequences[:, 1:]\n",
        "  return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "mbEk0oPaIGCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Transformer\n",
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.supports_masking = True\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=embed_dim\n",
        "        )\n",
        "\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=embed_dim\n",
        "        )\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"embed_dim\": self.embed_dim,\n",
        "      \"num_heads\": self.num_heads,\n",
        "      \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1),\n",
        "         tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    casual_mask = self.get_causal_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(\n",
        "          mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, casual_mask)\n",
        "    else:\n",
        "      padding_mask = mask\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs,\n",
        "        value=inputs,\n",
        "        key=inputs,\n",
        "        attention_mask=casual_mask\n",
        "    )\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(\n",
        "        query=attention_output_1,\n",
        "        value=encoder_outputs,\n",
        "        key=encoder_outputs,\n",
        "        attention_mask=padding_mask)\n",
        "    attention_output_2 = self.layernorm_2(\n",
        "        attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.supports_masking = True\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim,\n",
        "                                             output_dim=output_dim,\n",
        "                                             mask_zero=True)\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length,\n",
        "                                                output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return self.token_embeddings.compute_mask(inputs)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "gVjQ4_3fJHVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer approach\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ],
      "metadata": {
        "id": "qZCdfScgII4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Text generation callback\n",
        "\"\"\"\n",
        "Callback to generate range of text using different temperatures after every epoch\n",
        "Goal: Observe the evolution of the generated text as the  model begins converge.\n",
        "\n",
        "Method: Seed text \"this movie\"\n",
        "\"\"\"\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "  \"\"\"\n",
        "  Implements variable - temperature sampling form a probability distribution.\n",
        "  \"\"\"\n",
        "  predictions = np.asarray(predictions).astype(\"float64\")\n",
        "  predictions = np.log(predictions) / temperature\n",
        "  exp_preds = np.exp(predictions)\n",
        "  predictions = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, predictions, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  def __init__(\n",
        "      self,\n",
        "      prompt, # Prompt to seed\n",
        "      generate_length, # How many words to generate\n",
        "      model_input_length,\n",
        "      temperatures=(1.,), # Range of temp to use for sampling\n",
        "      print_freq=1):\n",
        "    self.prompt = prompt\n",
        "    self.generate_length = generate_length\n",
        "    self.model_input_length = model_input_length\n",
        "    self.temperatures = temperatures\n",
        "    self.print_freq = print_freq\n",
        "    # Compute length of tokenized input. To offset when sampling next token.\n",
        "    # What shape is this here? What is happening?????\n",
        "    vectorized_prompt = text_vectorization([prompt])[0].numpy()\n",
        "    self.prompt_length = np.nonzero(vectorized_prompt == 0)[0][0]\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if (epoch + 1) % self.print_freq != 0:\n",
        "      return\n",
        "    for temperature in self.temperatures:\n",
        "      print(\"== Generating with temperature\", temperature)\n",
        "      # Start from prompt\n",
        "      sentence = self.prompt\n",
        "      for i in range(self.generate_length):\n",
        "        tokenized_sentence = text_vectorization([sentence])\n",
        "        # Feed current sequence into model\n",
        "        predictions = self.model(tokenized_sentence)\n",
        "        # Retrieve the pred for the last timestep and use them to sample new word\n",
        "        next_token = sample_next(\n",
        "            predictions[0, self.prompt_length - 1 + i, :]\n",
        "        )\n",
        "        sampled_token = tokens_index[next_token]\n",
        "        # Append the new word to the current sequence\n",
        "        sentence += \" \" + sampled_token\n",
        "      print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ],
      "metadata": {
        "id": "XAyJ4VO2IM9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE-5LPtoIPRQ",
        "outputId": "c4b19424-31a5-41cb-aab7-a943f7ad6a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - loss: 6.3879== Generating with temperature 0.2\n",
            "This movie is pretty weak plot nicely from the rip off theres a has cute movie right sniper way to possibly an interesting to figure towards the actor as money a band foolish life day is panther not sure the same [UNK] in the elevator and his parents theirs and all i\n",
            "== Generating with temperature 0.5\n",
            "This movie is the cinema kills a series character was mostly entirely around him this is one hayek tragic and his [UNK] they are good style they might have blame her mark in pamela loose but this list with those chanting aside from the devil series having to be wonder what properly\n",
            "== Generating with temperature 0.7\n",
            "This movie has to save this however is one conventions rash very candidate which pearls tragic film wasnt quaint rubbish the story however at all great reason why didnt werent like the bad theaters i suspect something else on cable and this same fact the swords comedy no award inheritance counterpoint and\n",
            "== Generating with temperature 1.0\n",
            "This movie has to die good movie its stereotypes i ended out as far more than a tape as dull and steven papers into brilliant burt doe too many other extreme beloved few pictures leaving peter reunited but if youre nothing to be a point colorful one i have was made for\n",
            "== Generating with temperature 1.5\n",
            "This movie follows bits on hartley doesnt really finished watching if youre hoping every character horrible movie is hardly recommend oddly looking for the story one of us to nicks with all regarding after his own tokyo havoc connection could be a nonexistent if youre not desperate to slower in order to\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 423ms/step - loss: 6.3867\n",
            "Epoch 2/200\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - loss: 5.5013== Generating with temperature 0.2\n",
            "This movie is and what can say its based a movie that it is beautifully thank top all that made it would buy another are more it contains why is right if you have no like some of the distance with such a nightmare is cute very latest drama that its a\n",
            "== Generating with temperature 0.5\n",
            "This movie has ever since the best of the song any [UNK] you need for television it to be the acting and scared everyone said two [UNK] ball a kid who likes after a huge screwball film will be a voice and henry pacino it is bad film of its a small\n",
            "== Generating with temperature 0.7\n",
            "This movie was the worst movies i was that if anyone could have to me if we get away with reaction to be the [UNK] to show the wooden and all had come something a [UNK] part of this film to catch this movie house of person that are just left in\n",
            "== Generating with temperature 1.0\n",
            "This movie should be a shallow and the [UNK] and only one or a cool the second one of its a performance is bad as the [UNK] [UNK] and the realm of the backbone of what would sit through the best of itself is such a mixture of the only kind of\n",
            "== Generating with temperature 1.5\n",
            "This movie is not more than i remember ever to see it you what a movie it i liked it and not ever made romance at the [UNK] fall in saying much to regarded it 3 through the prone a mediocre and shows so tired of all its a extras i quickly\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 382ms/step - loss: 5.5012\n",
            "Epoch 3/200\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - loss: 5.3410== Generating with temperature 0.2\n",
            "This movie may be a bad it for [UNK]  as   even and  this instead should are dont  effects  and it   if  in with and this i    in was   on in the   to   just have\n",
            "== Generating with temperature 0.5\n",
            "This movie is one [UNK] i liked the only revealed this film i ever watched the best ever the william collapsing with a kid was an ad it does i do a cliché and i did you have proudly brothers running when it if i used to life in this show ever\n",
            "== Generating with temperature 0.7\n",
            "This movie to me laugh but there was totally lost kids played handled poorly written the money because note the true to film it please mock  and  please go by  but  with  claim and could that but unless veteran very watch is then for in to \n",
            "== Generating with temperature 1.0\n",
            "This movie   but tim sometimes it exciting i   but   but  was   and and there and   for   has expect would   in and to    dialogue   never complete  i level   love \n",
            "== Generating with temperature 1.5\n",
            "This movie provided not worth a lack a ride 1 everyday someone to watch this this film though a movie its a [UNK]  and dont  and   was     and starts so need in   its especially now  before   interests  with\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 381ms/step - loss: 5.3409\n",
            "Epoch 4/200\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 5.2356== Generating with temperature 0.2\n",
            "This movie was time i dont drink laugh thats advertised anyway just bad over and just plain bad the movie could tell me laugh involving some interesting the plot this  was  and in but this i delivering  i so i i rent three about by and indeed this please\n",
            "== Generating with temperature 0.5\n",
            "This movie is good as its been humorous nudity and really hilarious and highly insane its worth expected   youll and   completely sometimes track talent  scene this  instead dialogue i nor normally i and giving unless this ive even unfortunately very murders and in about  and\n",
            "== Generating with temperature 0.7\n",
            "This movie was the perfect if   just but to  this as   im 1  i    adds [UNK]   by  cartoons but but the the and before 2  quality if  look  one i was at and  when right the\n",
            "== Generating with temperature 1.0\n",
            "This movie will stand [UNK] presentation of all 20 min is no [UNK] to say that should be slasher of garbage    ending  i  the as  do  but entire then  by trick  and might to to rent for though did the but i and\n",
            "== Generating with temperature 1.5\n",
            "This movie better than [UNK] movie making for years  this  this by   but but and  because with is with first funny borrows and and and but and and even to i  look i but  if and and i  look is  in maybe and\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 382ms/step - loss: 5.2355\n",
            "Epoch 5/200\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 5.1600== Generating with temperature 0.2\n",
            "This movie can forget an honorable bette  1 and segment until the many the every in by movie the but and also in script by all and this with the because the this the jean  when and they [UNK] series i two i anyone the find mainly it there in\n",
            "== Generating with temperature 0.5\n",
            "This movie has several times and well beginning with it is an enjoyable moments that  and theres  look and the this this in is i as and and by but all choices french and and and soundtrack and and or this two watching wonderful to base as and and extras\n",
            "== Generating with temperature 0.7\n",
            "This movie begins somewhere with absolutely no one doesnt even worth watching  this plot the without the about this from the as unfortunately actors actors even [UNK] movie if and [UNK] the as there episodes yes out at when [UNK] acting and the the and and by even and editing they\n",
            "== Generating with temperature 1.0\n",
            "This movie must see jackie aubrey smith using a very beautiful actress  and even writing this it i in and lots but the version by because and may with the even but if look in in the this and this and viewer and plus and in wore i before all with\n",
            "== Generating with temperature 1.5\n",
            "This movie have gimmicks on a scary painting it will be a monkey or lovable fragile entertainment for anyone who [UNK]  not in but youll tv but the leave by i the on as and dvd the and weve over trailer  with and the the if and unless this the\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 382ms/step - loss: 5.1600\n",
            "Epoch 6/200\n",
            "\u001b[1m282/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 353ms/step - loss: 5.1022"
          ]
        }
      ]
    }
  ]
}